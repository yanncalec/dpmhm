{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19108fd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SimCLR and Few-shot Learning\n",
    "\n",
    "We train a SimCLR base model on a meta-dataset composed of DIRG+Paderborn, then transfer it to a CWRU few-shot dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2594b9-86f8-4878-8a7f-cf22385c41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %xmode minimal\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # disable GPU devices\n",
    "os.environ[\"TFDS_DATA_DIR\"] = os.path.expanduser(\"~/tensorflow_datasets\")  # default location of tfds database\n",
    "\n",
    "# Turn off logging for TF\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers, models\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "from keras import layers, models, ops, losses, metrics\n",
    "\n",
    "from keras.applications import resnet\n",
    "\n",
    "# from keras.applications import vgg16\n",
    "\n",
    "# tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4e7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dpmhm\n",
    "# dpmhm.datasets.get_dataset_list()\n",
    "\n",
    "from dpmhm.datasets import preprocessing, transformer, feature, utils, spectral_window_pipeline, spectral_pipeline\n",
    "from dpmhm.models import simclr\n",
    "\n",
    "workdir = Path(os.path.expanduser(\"~/tmp/dpmhm/SimCLR-fewshot\"))\n",
    "os.makedirs(workdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c5e43-be47-4cc8-9020-93e8e3ee35a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build the meta-dataset from DIRG and Paderborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76bacbc8-8b7d-4fc1-8724-2a77a0299536",
   "metadata": {},
   "outputs": [],
   "source": [
    "_func = lambda x, sr: feature.spectral_features(\n",
    "    x, sr, 'spectrogram',\n",
    "    # n_mfcc=256,\n",
    "    time_window=0.025, hop_step=0.0125,\n",
    "    # n_fft=1024,\n",
    "    normalize=False, to_db=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2315741c-6e81-4208-9c11-4b8824700c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_shape = (64,64)\n",
    "ds_all = {}\n",
    "\n",
    "foo = spectral_pipeline(\n",
    "    'DIRG', _func, \n",
    "    split='variation', compactor_kwargs=dict(channels=['A1']),\n",
    "    shuffle_files=True  # turn on shuffle at the file level\n",
    ")\n",
    "ds_all['DIRG_A1'] = utils.restore_cardinality(\n",
    "    foo[0].dataset,\n",
    "    foo[-1]\n",
    ")\n",
    "\n",
    "foo = spectral_pipeline(\n",
    "    'DIRG', _func, \n",
    "    split='variation', compactor_kwargs=dict(channels=['A2']),\n",
    "    shuffle_files=True  # turn on shuffle at the file level\n",
    ")\n",
    "ds_all['DIRG_A2'] = utils.restore_cardinality(\n",
    "    foo[0].dataset,\n",
    "    foo[-1]\n",
    ")\n",
    "\n",
    "foo = spectral_pipeline(\n",
    "    'Paderborn', _func,\n",
    "    split='healthy+artificial', compactor_kwargs=dict(channels=['vibration', 'current']),\n",
    "    # split='healthy[:25%]+artificial[:25%]', compactor_kwargs=dict(channels=['vibration', 'current']),\n",
    "    shuffle_files=True\n",
    ")\n",
    "ds_all['Paderborn'] = utils.restore_cardinality(\n",
    "    foo[0].dataset,\n",
    "    foo[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe9b7b8-ea8d-488f-9d2c-df06bd5d1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = transformer.SpecAugment(\n",
    "    ds_all['DIRG_A1'],\n",
    "    output_shape=window_shape,\n",
    "    # blur_kwargs={'prob':0},\n",
    "    # fade_kwargs={'prob':0},\n",
    "    # flip_kwargs={'prob':0}\n",
    ").dataset.map(lambda x: x['feature'])\n",
    "\n",
    "ds2 = transformer.SpecAugment(\n",
    "    ds_all['DIRG_A2'],\n",
    "    output_shape=window_shape,\n",
    ").dataset.map(lambda x: x['feature'])\n",
    "\n",
    "ds3 = transformer.SpecAugment(\n",
    "    ds_all['Paderborn'],\n",
    "    output_shape=window_shape,\n",
    ").dataset.map(lambda x: x['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899558ff-dea9-4a9f-a019-c11bf79091df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(476, shape=(), dtype=int64) TensorSpec(shape=(3, 64, 64), dtype=tf.float32, name=None)\n"
     ]
    }
   ],
   "source": [
    "# ds1, ds2, ds3 = ds_all['DIRG_A1'], ds_all['DIRG_A2'], ds_all['Paderborn']\n",
    "ds0 = ds1.concatenate(ds2).concatenate(ds3.take(ds1.cardinality()+ds2.cardinality()))\n",
    "# ds0 = ds1.concatenate(ds2).concatenate(ds3)\n",
    "\n",
    "print(ds0.cardinality(), ds0.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45369e10-5185-4287-b57e-6843b2e3b4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(64, 64, 3), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(64, 64, 3), dtype=tf.float32, name=None)),\n",
       " TensorSpec(shape=(), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds, input_shape = utils.twins_dataset_ssl(ds0, stack=False, fake_label=True)\n",
    "\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda52072-c2b5-4644-b5cb-91892be7dcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = utils.restore_cardinality(ds, ds0.cardinality())\n",
    "ds_size = int(ds.cardinality())  # utils.get_dataset_size(ds)\n",
    "\n",
    "ds_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf217b1-20bd-428f-9fd7-db5f6447ee12",
   "metadata": {},
   "source": [
    "## Base SimCLR model\n",
    "\n",
    "We train a base SimCLR model on the meta-dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22294c27-1e4a-4601-b653-c19b3f272f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(32, 64, 64, 3), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(32, 64, 64, 3), dtype=tf.float32, name=None)),\n",
       " TensorSpec(shape=(32,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {'train':0.8, 'val':0.2}\n",
    "\n",
    "ds_split = utils.split_dataset(ds, splits, ds_size=ds_size)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "ds_train = ds_split['train']\\\n",
    "    .shuffle(ds_size, reshuffle_each_iteration=True)\\\n",
    "    .batch(batch_size, drop_remainder=True)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = ds_split['val'].batch(batch_size, drop_remainder=True)\n",
    "# ds_test = ds_split['test'].batch(1, drop_remainder=True)\n",
    "\n",
    "ds_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b55598a-4f7c-4905-acf7-108805617b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sim_clr_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sim_clr_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ functional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ projector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ functional_8 (\u001b[38;5;33mFunctional\u001b[0m)       │ ?                      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ projector (\u001b[38;5;33mSequential\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encoder_kwargs = dict(include_top=False, weights='imagenet', pooling='max')\n",
    "encoder_kwargs = dict(include_top=False, weights=None, pooling='avg')\n",
    "\n",
    "model = dpmhm.models.simclr.SimCLR(input_shape, name='VGG16', tau=0.1, **encoder_kwargs)\n",
    "model._encoder.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddcef86c-417d-4e11-b43c-7bcf3dc254a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 5s/step - loss: 44742.3594 - val_loss: 47288.3281\n"
     ]
    }
   ],
   "source": [
    "hh = model.fit(ds_train,\n",
    "               validation_data=ds_val,\n",
    "               epochs=500)\n",
    "\n",
    "# plt.plot(hh.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2f9a6-5754-4247-a368-76ad2dc062ba",
   "metadata": {},
   "source": [
    "From the trained SimCLR model, we extract the feature transformation part which includes the base encoder and the first two dense layers of the projection head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f31cbb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SimCLR_feature\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"SimCLR_feature\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,166,464</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_8 (\u001b[38;5;33mFunctional\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_11 (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m3,166,464\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,881,152</span> (68.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,881,152\u001b[0m (68.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,872,960</span> (68.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,872,960\u001b[0m (68.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> (32.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,192\u001b[0m (32.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = layers.Input(input_shape)\n",
    "\n",
    "# same same\n",
    "# _proj = models.Model(inputs=model._projector.inputs, outputs=model._projector.layers[3].output)\n",
    "_proj = models.Model(inputs=model._projector.layers[0].input, outputs=model._projector.layers[3].output)\n",
    "\n",
    "# _proj.summary()  # shows a concrete value for batch\n",
    "\n",
    "f = _proj(model._encoder(x))\n",
    "\n",
    "model_feature = models.Model(inputs=x, outputs=f, name='SimCLR_feature')\n",
    "\n",
    "model_feature.summary()  # shows `None` for batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832377ed-b314-4011-9155-50058405330b",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfcc90c3-8957-4eca-80df-3895de408c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all['CWRU'], full_labels_dict = spectral_window_pipeline(\n",
    "    'CWRU', _func,\n",
    "    split='all',\n",
    "    compactor_kwargs=dict(keys=['FaultLocation', 'FaultComponent', 'FaultSize']),\n",
    "    window_kwargs=dict(window_size=(64,64), hop_size=(64,64))\n",
    ")\n",
    "\n",
    "labels = list(full_labels_dict.keys())  \n",
    "n_classes = len(labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b4760-e041-443c-9abb-76d36505aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preprocessing.get_mapping_supervised(labels)\n",
    "\n",
    "dw = utils.restore_cardinality(\n",
    "    utils.restore_shape(\n",
    "        ds_all['CWRU'].map(preproc, num_parallel_calls=tf.data.AUTOTUNE),\n",
    "        key=0\n",
    "    )\n",
    ")\n",
    "\n",
    "dw_size = int(dw.cardinality())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "855b5e7c-c8d5-4571-99e6-b3c5c04bdb13",
   "metadata": {},
   "source": [
    "### Supervised fine tuning\n",
    "\n",
    "We add a classification head to the feature transformation network and fine tune the model on some new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "717a9296-8be7-4778-8449-b6733f8ad454",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train':0.7, 'val':0.2, 'test':0.1}\n",
    "batch_size = 64\n",
    "\n",
    "dw_split = utils.split_dataset(\n",
    "    dw, splits, \n",
    "    ds_size=dw_size, \n",
    "    # labels=np.arange(n_classes)\n",
    ")\n",
    "\n",
    "dw_train = dw_split['train']\\\n",
    "    .shuffle(dw_size, reshuffle_each_iteration=True)\\\n",
    "    .batch(batch_size, drop_remainder=True)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "dw_val = dw_split['val'].repeat().batch(batch_size, drop_remainder=True)\n",
    "dw_test = dw_split['test'].batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28bca4-051a-48ab-8781-884798e9ea8b",
   "metadata": {},
   "source": [
    "The classification head here is a simple MLP. The weights of the feature transformation network are frozen for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99f13cd1-5223-4bc8-bdbd-51bc28227657",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature.trainable = False\n",
    "\n",
    "class_head = models.Sequential([\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(n_classes) # nb labels\n",
    "], name='Classification_head')\n",
    "\n",
    "x = layers.Input(input_shape)\n",
    "\n",
    "model_fine = models.Model(inputs=x, outputs=class_head(model_feature(x)))\n",
    "\n",
    "model_fine.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03fca33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - loss: 3.1987 - sparse_categorical_accuracy: 0.0869"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hh = model_fine.fit(\n",
    "    dw_train,\n",
    "    validation_data=dw_val,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2cabf96a-e1b5-4f89-8803-f3e5828f8127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 305ms/step - loss: 3.0477 - sparse_categorical_accuracy: 0.0944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.059857130050659, 0.09749999642372131]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fine.evaluate(dw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ed300-8f74-4b4a-8021-a3a0ff0dd77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8690f2-51b4-4d1d-932d-f811dd0716e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c933834b-8f37-4d09-aa08-857c028148da",
   "metadata": {},
   "source": [
    "# EOF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43cb9215-3271-4fbc-8cb7-d811eb46fa44",
   "metadata": {},
   "source": [
    "### Few-shot learning\n",
    "\n",
    "In few-shot learning the number of new data per category is limited. We can prepare the data for few-shot learning by splitting separately data of each category.\n",
    "\n",
    "However for unknown reasons, the performance of the few-shot split seems to be very low compared to the normal split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4d286-ae74-4374-b024-bc56802185d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train':0.2, 'val':0.7, 'test':0.1}\n",
    "batch_size = 64\n",
    "\n",
    "n_classes = len(labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1900905-2159-4730-811c-1cad58aed4c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only for demonstration, here we apply the preprocessing after the split.\n",
    "dw_split = utils.split_dataset(\n",
    "    dw, splits, \n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "for k, dv in dw_split.items():\n",
    "    dv.save(str(workdir/f'fs_split_{k}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "da1feab5-99b4-47bf-b74a-65635826a62b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dw_split = {}\n",
    "for k in splits.keys():\n",
    "    dw_split[k] = tf.data.Dataset.load(str(workdir/f'fs_split_{k}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cd30de0d-b0e0-4f39-8be6-3dbef83e7571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dw_train = dw_split['train']\\\n",
    "    .map(preproc, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "    .shuffle(1000, reshuffle_each_iteration=True)\\\n",
    "    .batch(batch_size, drop_remainder=True)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "dw_val = dw_split['val']\\\n",
    "    .map(preproc, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "dw_test = dw_split['test']\\\n",
    "    .map(preproc, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "    .batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d28dcbc1-3624-4287-a9d1-aa2c439b6034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 ms, sys: 13.4 ms, total: 185 ms\n",
      "Wall time: 58.7 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 23:51:57.085419: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "%time eles = list(dw_train.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36605327-877a-4028-835d-1a03a2ee7610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eles[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922e20b-8274-4f63-8781-2d83536e7413",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature(eles[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "09665753-31d0-46bc-b4dd-62ea791ca6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature.trainable = False\n",
    "\n",
    "class_head = models.Sequential([\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(n_classes) # nb labels\n",
    "], name='Classification_head')\n",
    "\n",
    "x = layers.Input(input_shape)\n",
    "\n",
    "model_fs = models.Model(inputs=x, outputs=class_head(model_feature(x)))\n",
    "\n",
    "model_fs.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3990ecfe-8645-4cc1-9d2c-b926ae95d899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to automatically build the model. Please build it yourself before calling fit/evaluate/predict. A model is 'built' when its variables have been created and its `self.built` attribute is True. Usually, calling the model on a batch of data is the right way to build it.\nException encountered:\n'Exception encountered when calling Conv2D.call().\n\n\u001b[1m'NoneType' object is not callable\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=jnp.ndarray(shape=(64, 70, 70, 3), dtype=float32)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hh \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdw_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdw_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/cache/.cache/pypoetry/virtualenvs/dpmhm-yVS8YoI0-py3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/mnt/cache/.cache/pypoetry/virtualenvs/dpmhm-yVS8YoI0-py3.11/lib/python3.11/site-packages/keras/src/trainers/trainer.py:999\u001b[0m, in \u001b[0;36mTrainer._symbolic_build\u001b[0;34m(self, iterator, data_batch)\u001b[0m\n\u001b[1;32m    997\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mcompute_output_spec(\u001b[38;5;28mself\u001b[39m, x)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to automatically build the model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease build it yourself before calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit/evaluate/predict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA model is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuilt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m when its variables have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeen created and its `self.built` attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis True. Usually, calling the model on a batch \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof data is the right way to build it.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException encountered:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1008\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1009\u001b[0m     )\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compile_metrics_unbuilt:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Build all metric state with `backend.compute_output_spec`.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     backend\u001b[38;5;241m.\u001b[39mcompute_output_spec(\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics,\n\u001b[1;32m   1014\u001b[0m         x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1018\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to automatically build the model. Please build it yourself before calling fit/evaluate/predict. A model is 'built' when its variables have been created and its `self.built` attribute is True. Usually, calling the model on a batch of data is the right way to build it.\nException encountered:\n'Exception encountered when calling Conv2D.call().\n\n\u001b[1m'NoneType' object is not callable\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=jnp.ndarray(shape=(64, 70, 70, 3), dtype=float32)'"
     ]
    }
   ],
   "source": [
    "hh = model_fs.fit(\n",
    "    dw_train,\n",
    "    validation_data=dw_val,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1029de-e257-4da5-a740-4e701503930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 23:41:21.174297: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:22.954204: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:24.714093: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:26.471666: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:28.231720: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:30.062777: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:31.896178: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:33.754727: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:35.616732: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:37.484919: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:39.284917: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:41.137899: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:42.974720: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:44.735700: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:46.547159: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:48.315764: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:50.082965: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:51.906632: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:53.679263: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:55.533645: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:57.320201: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:41:59.171508: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:42:00.959659: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-06-18 23:42:02.749389: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "model_fs.evaluate(dw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7e8b5-6d23-4b77-b9ec-1ee0ec1431d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dpmhm-yVS8YoI0-py3.11)",
   "language": "python",
   "name": "dpmhm-yvs8yoi0-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
