{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19108fd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MoCo\n",
    "\n",
    "Using the torch backend with GPU device may result in some strange error: dataloader seems to generate elements of random size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2594b9-86f8-4878-8a7f-cf22385c41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %xmode minimal\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # disable GPU devices\n",
    "os.environ[\"TFDS_DATA_DIR\"] = os.path.expanduser(\"~/tensorflow_datasets\")  # default location of tfds database\n",
    "\n",
    "# Turn off logging for TF\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO) \n",
    "# logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# import librosa\n",
    "# import librosa.display\n",
    "# from IPython.display import Audio\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "import keras\n",
    "from keras import layers, models, ops, losses, metrics\n",
    "# from keras.applications import resnet, vgg16\n",
    "\n",
    "# tf.config.experimental_run_functions_eagerly(True)\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "# import torch\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d2156e-ff0a-4fcc-9583-d0a6243df559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dpmhm\n",
    "# dpmhm.datasets.get_dataset_list()\n",
    "\n",
    "from dpmhm.datasets import preprocessing, transformer, feature, utils, spectral_window_pipeline, spectral_pipeline\n",
    "from dpmhm.models import simclr\n",
    "\n",
    "workdir = Path(os.path.expanduser(\"~/tmp/dpmhm/MoCo\"))\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "dbdir = Path(os.path.expanduser('~/Projects/HIASCI/Data/MetaTwins'))\n",
    "os.makedirs(dbdir, exist_ok=True)\n",
    "\n",
    "# dpmhm.datasets.query_parameters('Paderborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c5e43-be47-4cc8-9020-93e8e3ee35a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build a meta-dataset\n",
    "\n",
    "First we load several datasets and extract spectrogram patches of fixed dimension `(128,128)`. This dimension is large enough to accomodate the random crop of shape `(64,64)` that will be created later by spec-augmentation. Also we skip the resampling step which will considerable slow down the loading of datasets, and take randomly the first `Nmax` elements to reduce the size of the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460a13f-7844-4b58-b4b9-af4b7697cf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/han/tensorflow_datasets/dirg/1.0.0\n",
      "INFO:absl:Reusing dataset dirg (/home/han/tensorflow_datasets/dirg/1.0.0)\n",
      "INFO:absl:Creating a tf.data.Dataset reading 16 files located in folders: /home/han/tensorflow_datasets/dirg/1.0.0.\n",
      "2024-07-25 10:38:36.273431: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "INFO:absl:Constructing tf.data.Dataset dirg for split variation, from /home/han/tensorflow_datasets/dirg/1.0.0\n"
     ]
    }
   ],
   "source": [
    "n_fft = 1024  # number of frequency bins\n",
    "Nmax = 2500  # maximum number of elements per dataset\n",
    "shuffle_size = 10000\n",
    "\n",
    "ds_all = {}\n",
    "\n",
    "foo = dpmhm.datasets.spectral_window_pipeline('DIRG', 51200, split='variation', channels=[], n_fft=n_fft)\n",
    "# ds_all['DIRG'] = foo\n",
    "ds_all['DIRG'] = utils.restore_cardinality(foo.shuffle(shuffle_size).take(Nmax))\n",
    "\n",
    "foo = dpmhm.datasets.spectral_window_pipeline('Paderborn', 64000, split='healthy[:10%]+artificial[:10%]', channels=['vibration'], n_fft=n_fft)\n",
    "# ds_all['Paderborn'] = foo\n",
    "ds_all['Paderborn'] = utils.restore_cardinality(foo.shuffle(shuffle_size).take(Nmax))\n",
    "\n",
    "foo = dpmhm.datasets.spectral_window_pipeline('Ottawa', 200000, split='all', channels=[], n_fft=n_fft)\n",
    "# ds_all['Ottawa'] = foo\n",
    "ds_all['Ottawa'] = utils.restore_cardinality(foo.shuffle(shuffle_size).take(Nmax))\n",
    "\n",
    "foo = dpmhm.datasets.spectral_window_pipeline('Phmap2021', 10544, split='train[:50%]', channels=[], n_fft=n_fft)\n",
    "# ds_all['Phmap2021'] = foo\n",
    "ds_all['Phmap2021'] = utils.restore_cardinality(foo.shuffle(shuffle_size).take(Nmax))\n",
    "\n",
    "# eles = list(foo.take(10).as_numpy_iterator())\n",
    "\n",
    "# ds_size = 0\n",
    "# for k, foo in ds_all.items():\n",
    "#     print(k, int(foo.cardinality()))\n",
    "#     ds_size += int(foo.cardinality())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee7f6d-46b4-43b4-a6a2-4dc2e5392870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialization for better performance.\n",
    "# Note that this should be done before the random augmentation.\n",
    "\n",
    "ds1, ds2, ds3, ds4 = ds_all['DIRG'], ds_all['Paderborn'], ds_all['Ottawa'], ds_all['Phmap2021']\n",
    "\n",
    "ds0 = ds1.concatenate(ds2).concatenate(ds3).concatenate(ds4)\n",
    "\n",
    "ds0.save(str(dbdir/'dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db26e23-501b-4e91-82d4-886be299d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 15:11:09.406567: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(64, 64, 1), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(64, 64, 1), dtype=tf.float32, name=None)),\n",
       " TensorSpec(shape=(), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds0 = tf.data.Dataset.load(str(dbdir/'dataset'))\n",
    "\n",
    "window_shape = (64, 64)\n",
    "\n",
    "dt = transformer.SpecAugmentTwins(\n",
    "    ds0,\n",
    "    output_shape=window_shape,\n",
    "    crop_kwargs={'prob':0.5},\n",
    "    flip_kwargs={'axis':-1, 'prob':0.5},\n",
    "    blur_kwargs={'sigma':1., 'prob':0.},\n",
    "    fade_kwargs={'prob':0},\n",
    ").dataset.map(\n",
    "    lambda y1, y2: (tf.transpose(y1, perm=(1,2,0)), tf.transpose(y2, perm=(1,2,0)))  # to channel-last\n",
    ")\n",
    "\n",
    "ds = tf.data.Dataset.zip(dt, utils.constant_dataset())\n",
    "\n",
    "# ds = utils.restore_cardinality(ds, ds_size)\n",
    "input_shape = ds.element_spec[0][0].shape\n",
    "\n",
    "ds_size = int(dt.cardinality())\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26fb842-f52b-4995-89c1-dd8b6e3b7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Serialization may disable the random augmentation\n",
    "\n",
    "# ds.save(str(workdir/'simclr_dataset'))\n",
    "# ds = tf.data.Dataset.load(str(workdir/'simclr_dataset'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf217b1-20bd-428f-9fd7-db5f6447ee12",
   "metadata": {},
   "source": [
    "## Base MoCo model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22294c27-1e4a-4601-b653-c19b3f272f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(32, 64, 64, 1), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(32, 64, 64, 1), dtype=tf.float32, name=None)),\n",
       " TensorSpec(shape=(32,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "shuffle_size = 10000\n",
    "\n",
    "# ds_train = ds.batch(batch_size, drop_remainder=True)\n",
    "ds_train = ds.shuffle(shuffle_size, reshuffle_each_iteration=True)\\\n",
    "    .repeat()\\\n",
    "    .batch(batch_size, drop_remainder=True)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62cd79b-c74a-4341-a147-c016f20aad11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mo_co\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"mo_co\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ functional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,718</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ projector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ online (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ online (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ functional_2 (\u001b[38;5;33mFunctional\u001b[0m)       │ ?                      │    \u001b[38;5;34m23,587,718\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ projector (\u001b[38;5;33mSequential\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ online (\u001b[38;5;33mSequential\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ online (\u001b[38;5;33mSequential\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,994,636</span> (183.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,994,636\u001b[0m (183.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,534,598</span> (89.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,534,598\u001b[0m (89.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,460,038</span> (93.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m24,460,038\u001b[0m (93.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encoder_kwargs = dict(include_top=False, weights='imagenet', pooling='avg')\n",
    "encoder_kwargs = dict(include_top=False, weights=None, pooling='avg')\n",
    "\n",
    "model = dpmhm.models.MoCo(\n",
    "    input_shape, \n",
    "    name='ResNet50', tau=0.1, \n",
    "    memsize=100*32, encoder_kwargs=encoder_kwargs\n",
    ")\n",
    "\n",
    "model._encoder.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    ")\n",
    "cb_ema = dpmhm.models.MoCo_Callback()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# # Manually build the model, most of time not necessary\n",
    "# eles = list(ds_train.take(1))\n",
    "# model(eles[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d22744-8004-4ed8-aee1-ed5138ac3c23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dpmhm.models.ssl.moco:Create EMA instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 1s/step - loss: 205.4393\n",
      "Epoch 2/100\n",
      "\u001b[1m179/312\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 1s/step - loss: 226.0592"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hh \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m               \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcb_ema\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m               \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model.save(workdir/'moco_base.keras')\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/cache/.cache/pypoetry/virtualenvs/dpmhm-yVS8YoI0-py3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/mnt/cache/.cache/pypoetry/virtualenvs/dpmhm-yVS8YoI0-py3.11/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:254\u001b[0m, in \u001b[0;36mTorchTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 254\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/cache/.cache/pypoetry/virtualenvs/dpmhm-yVS8YoI0-py3.11/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:117\u001b[0m, in \u001b[0;36mTorchTrainer.make_train_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/cache/.cache/pypoetry/virtualenvs/dpmhm-yVS8YoI0-py3.11/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:65\u001b[0m, in \u001b[0;36mTorchTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_weights:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Call torch.Tensor.backward() on the loss to compute gradients\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# for the weights.\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     trainable_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_weights[:]\n\u001b[1;32m     68\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m [v\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m trainable_weights]\n",
      "File \u001b[0;32m/mnt/cache/.cache/pypoetry/virtualenvs/dpmhm-yVS8YoI0-py3.11/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/cache/.cache/pypoetry/virtualenvs/dpmhm-yVS8YoI0-py3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/cache/.cache/pypoetry/virtualenvs/dpmhm-yVS8YoI0-py3.11/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hh = model.fit(ds_train,\n",
    "               callbacks=[cb_ema],\n",
    "               steps_per_epoch=ds_size // batch_size,\n",
    "               epochs=100)\n",
    "\n",
    "model.save(workdir/'moco_base.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a41385-74e1-4696-81d4-538416db41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(model.history.history['loss'])\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "fig.savefig(workdir/'convergence.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2f9a6-5754-4247-a368-76ad2dc062ba",
   "metadata": {},
   "source": [
    "From the trained model, we extract the feature transformation part which includes the base encoder and the first two dense layers of the projection head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a9ef99-19de-4186-8df4-9c6127753012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Moco_feature\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Moco_feature\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,718</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,364,672</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_2 (\u001b[38;5;33mFunctional\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │    \u001b[38;5;34m23,587,718\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_11 (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m2,364,672\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,952,390</span> (99.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,952,390\u001b[0m (99.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,897,222</span> (98.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,897,222\u001b[0m (98.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,168</span> (215.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m55,168\u001b[0m (215.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = layers.Input(input_shape)\n",
    "\n",
    "# same same\n",
    "# _proj = models.Model(inputs=model._projector.inputs, outputs=model._projector.layers[3].output)\n",
    "_proj = models.Model(inputs=model._projector.layers[0].input, outputs=model._projector.layers[3].output)\n",
    "\n",
    "# _proj.summary()  # shows a concrete value for batch\n",
    "\n",
    "f = _proj(model._encoder(x))\n",
    "\n",
    "model_feature = models.Model(inputs=x, outputs=f, name='Moco_feature')\n",
    "\n",
    "model_feature.summary()  # shows `None` for batch\n",
    "\n",
    "# model_feature.save(str(workdir/'simclr_feature.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832377ed-b314-4011-9155-50058405330b",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "192c463d-0ba9-4f1b-9c2e-879cc4a20dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/han/tensorflow_datasets/cwru/1.0.0\n",
      "INFO:absl:Reusing dataset cwru (/home/han/tensorflow_datasets/cwru/1.0.0)\n",
      "INFO:absl:Creating a tf.data.Dataset reading 4 files located in folders: /home/han/tensorflow_datasets/cwru/1.0.0.\n",
      "INFO:absl:Constructing tf.data.Dataset cwru for split all, from /home/han/tensorflow_datasets/cwru/1.0.0\n"
     ]
    }
   ],
   "source": [
    "foo, full_labels_dict = dpmhm.datasets.spectral_window_pipeline(\n",
    "    'CWRU', 12000, split='all', \n",
    "    channels=[], \n",
    "    keys=['FaultLocation', 'FaultComponent', 'FaultSize'], \n",
    "    n_fft=1024, ws=64, labels=True\n",
    ")\n",
    "\n",
    "labels = list(full_labels_dict.keys())  \n",
    "n_classes = len(labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec6b4760-e041-443c-9abb-76d36505aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preprocessing.get_mapping_supervised(labels)\n",
    "\n",
    "dw = utils.restore_cardinality(\n",
    "    utils.restore_shape(\n",
    "        foo.map(preproc, num_parallel_calls=tf.data.AUTOTUNE),\n",
    "        key=0\n",
    "    )\n",
    ")\n",
    "\n",
    "dw_size = int(dw.cardinality())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "855b5e7c-c8d5-4571-99e6-b3c5c04bdb13",
   "metadata": {},
   "source": [
    "### Supervised training of the classification head\n",
    "\n",
    "We add a classification head to the feature transformation network and fine tune the model on some new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "717a9296-8be7-4778-8449-b6733f8ad454",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train':0.7, 'val':0.2, 'test':0.1}\n",
    "batch_size = 32\n",
    "\n",
    "dw_split = utils.split_dataset(\n",
    "    dw, splits, \n",
    "    ds_size=dw_size, \n",
    "    # labels=np.arange(n_classes)\n",
    ")\n",
    "\n",
    "dw_train = dw_split['train']\\\n",
    "    .shuffle(dw_size, reshuffle_each_iteration=True)\\\n",
    "    .repeat()\\\n",
    "    .batch(batch_size, drop_remainder=True)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "dw_val = dw_split['val'].batch(batch_size, drop_remainder=True)\n",
    "dw_test = dw_split['test'].batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28bca4-051a-48ab-8781-884798e9ea8b",
   "metadata": {},
   "source": [
    "The classification head here is a simple MLP. The weights of the feature transformation network are frozen for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99f13cd1-5223-4bc8-bdbd-51bc28227657",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature.trainable = False\n",
    "\n",
    "class_head = models.Sequential([\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    # layers.BatchNormalization(),\n",
    "    layers.Dense(n_classes, activation=None) # nb labels\n",
    "], name='Classification_head')\n",
    "\n",
    "x = layers.Input(input_shape)\n",
    "\n",
    "model_fine = models.Model(inputs=x, outputs=class_head(model_feature(x)))\n",
    "\n",
    "model_fine.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85517e4-8783-4233-b47d-78e7040fdedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    710/Unknown \u001b[1m116s\u001b[0m 125ms/step - loss: 3.2900 - sparse_categorical_accuracy: 0.0719"
     ]
    }
   ],
   "source": [
    "hh = model_fine.fit(\n",
    "    dw_train,\n",
    "    validation_data=dw_val,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659cf5bc-1f9e-4440-bd09-a8db0431c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine.evaluate(dw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8fc623-6aa1-4421-bb2e-ce9c3c9300ed",
   "metadata": {},
   "source": [
    "#### Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dceb8df-d7b4-4526-8822-a6134a655ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature.trainable = True\n",
    "\n",
    "model_fine.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "hh = model_fine.fit(\n",
    "    dw_train,\n",
    "    validation_data=dw_val,\n",
    "    epochs=2\n",
    ")\n",
    "\n",
    "model_fine.evaluate(dw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afaca99-9ea8-42ea-9cca-62a55dc9292d",
   "metadata": {},
   "source": [
    "# EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e057874-d3fa-4a43-8c5b-5b23ff568825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "array([0.6931472, 0.6931472, 0.6931472, 0.6931472, 0.6931472, 0.6931472,\n",
       "       0.6931472, 0.6931472, 0.6931472, 0.6931472, 0.6931472, 0.6931472,\n",
       "       0.6931472, 0.6931472, 0.6931472, 0.6931472, 0.6931472, 0.6931472,\n",
       "       0.6931472, 0.6931472, 0.6931472, 0.6931472, 0.6931472, 0.6931472,\n",
       "       0.6931472, 0.6931472, 0.6931472, 0.6931472, 0.6931472, 0.6931472,\n",
       "       0.6931472, 0.6931472], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = keras.random.normal((32,256))\n",
    "Y = keras.random.normal((32,256))\n",
    "# K = keras.random.normal((1, 32,256))\n",
    "K = ops.expand_dims(Y, 0)\n",
    "tau = 0.1\n",
    "axis = -1\n",
    "S = losses.cosine_similarity(X, Y, axis=axis) / tau  # X and Y have the same dimension, no need for broadcast. Result has shape `batch`.\n",
    "N = losses.cosine_similarity(X, K, axis=axis) /  tau  # has shape `(memlen, batch)`\n",
    "M = ops.transpose(ops.vstack([ops.expand_dims(S,0), N]))\n",
    "L = losses.sparse_categorical_crossentropy(\n",
    "    ops.zeros_like(S),  # batch size\n",
    "    M,  # has shape `(batch, memlen+1)`\n",
    "    from_logits=True\n",
    ")\n",
    "L\n",
    "# -ops.sum(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d965d-6abc-4e19-83f2-983105177d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dpmhm-yVS8YoI0-py3.11)",
   "language": "python",
   "name": "dpmhm-yvs8yoi0-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
